{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b246b6ac",
   "metadata": {},
   "source": [
    "## A.I. Assignment 4\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "* Get familiar with tensors in pytorch\n",
    "* Get familiar with the activation functions for ANN \n",
    "* Create a simple perceptron model with pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247d8bb",
   "metadata": {},
   "source": [
    "## Common activation functions for ANN:\n",
    "\n",
    "##### Sigmoid:\n",
    "\n",
    "The sigmoid function is a popular choice for activation functions in neural networks. It has an $S-shaped$ curve:\n",
    "$$f(x) = \\frac{1}{1+e^{-x}}.$$\n",
    "\n",
    "It has a number of appealing qualities:\n",
    "\n",
    "1. *Nonlinearity*: Because the sigmoid function is nonlinear, it enables the neural network to simulate nonlinear interactions between inputs and outputs. A neural network would simply be a linear model without a nonlinear activation function like sigmoid, which would significantly restrict its capacity to describe complex relationships.\n",
    "\n",
    "1. *Smoothness*: As the sigmoid function is differentiable and smooth, its derivative exist at every point. This is significant because it makes it possible for neural network training techniques based on gradients (such as backpropagation) to perform well.\n",
    "\n",
    "1. *Boundedness*: The sigmoid function is bounded between 0 and 1, it means  its outputs can be interpreted as probabilities.  It is most useful in applications like binary classification, where the goal is to predict whether an input belongs to one of two classes.\n",
    "\n",
    "1. *Monotonicity*: The sigmoid function is monotonic, which means that its outputs are always increasing or always decreasing with respect to its inputs. This makes it easy to interpret the effect of changes in input variables on the output of the network.\n",
    "\n",
    "##### ReLU (Rectified Linear Unit):\n",
    "\n",
    "The ReLU function is defined as $$f(x) = max(0, x).$$\n",
    "\n",
    "It is a widely used activation function in deep learning due to its simplicity and effectiveness.\n",
    "\n",
    "##### Tanh (Hyperbolic Tangent):\n",
    "\n",
    "The $\\tanh$ function is similar to the sigmoid function but produces outputs in the interval $[-1, 1]$:  \n",
    "$$f(x) = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}.$$\n",
    "\n",
    "##### Softmax:\n",
    "\n",
    "The softmax function is commonly used in the output layer of a neural network for multi-class classification problems. It normalizes the output into a probability distribution over the classes.\n",
    "\n",
    "Given a vector $\\vec{z}$ of $n$ real numbers, the softmax function calculates a vector $\\vec{s}$ of $n$ real numbers with the components:\n",
    "$$s_j = \\frac{e^{z_j}}{\\sum_{k=1}^{n} {e^{z_k}}}.$$\n",
    "\n",
    "\n",
    "##### Leaky ReLU:\n",
    "\n",
    "The Leaky ReLU is a variation of the ReLU function that introduces a small non-zero gradient for negative inputs. It is defined as \n",
    "$$f(x) = max(0.01 \\cdot x, x).$$\n",
    "\n",
    "##### ELU (Exponential Linear Unit):\n",
    "\n",
    "The ELU function is another variation of the ReLU function that introduces a small negative saturation value for negative inputs. It is defined as \n",
    "\n",
    "$$ f(x) = \\biggl\\{ \\begin{matrix} x, & for & x > 0 \\\\\n",
    "                      \\alpha \\cdot (e^{x} - 1), & for & x \\leq 0 \\end{matrix}$$\n",
    "where $\\alpha$ is a hyperparameter.\n",
    "\n",
    "##### Swish:\n",
    "\n",
    "The Swish function is a recent activation function that is a smooth approximation of the ReLU function. It is defined as f(x) = x * sigmoid(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68931328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93017ce5",
   "metadata": {},
   "source": [
    "create a tensor with requires_grad=True to tell PyTorch to track gradients for this tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a14b6a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56340210",
   "metadata": {},
   "source": [
    "You can perform any operations on this tensor as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99cb5a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x ** 2 + 2 * x + 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a18dc",
   "metadata": {},
   "source": [
    "To compute the gradients of y with respect to x, you need to call backward() on y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c244acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9b7e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ce525b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with requires_grad=True\n",
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "\n",
    "# Compute a function of x\n",
    "y = x.sum()\n",
    "\n",
    "# Compute gradients of y with respect to x\n",
    "y.backward()\n",
    "\n",
    "# Print gradients of x\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30804b8c",
   "metadata": {},
   "source": [
    "Exercise 1.\n",
    "\n",
    "Compute the gradient for the sigmoid activation function in 2 points using pytorch and check it with the known explicit formula "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dc94902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1966, 0.1050])\n",
      "tensor([0.1966, 0.1050], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "y = torch.sigmoid(x)\n",
    "y.sum().backward()\n",
    "grad_torch = x.grad\n",
    "print(grad_torch) \n",
    "\n",
    "y = 1 / (1 + torch.exp(-x))\n",
    "grad_formula = y * (1 - y)\n",
    "\n",
    "print(grad_formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e77a45c",
   "metadata": {},
   "source": [
    "Exercise 2.\n",
    "\n",
    "Compute the gradient for the linear activation function in 2 points using pytorch and check it with the known explicit formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7054039e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1.])\n",
      "tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "y = x\n",
    "y.sum().backward()\n",
    "grad_torch = x.grad\n",
    "print(grad_torch) \n",
    "\n",
    "grad_formula = torch.ones_like(x)\n",
    "print(grad_formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab117e3",
   "metadata": {},
   "source": [
    "Execise 3.\n",
    "\n",
    "Compute the gradient for the relu activation function in 2 points using pytorch and check it with the known explicit formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f69f4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.0000, 0.5000])\n",
      "tensor([1.0000, 0.0000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0, -1.5, 0], requires_grad=True)\n",
    "\n",
    "y = torch.max(torch.zeros_like(x), x)\n",
    "y.sum().backward()\n",
    "grad_torch = x.grad\n",
    "print(grad_torch)\n",
    "\n",
    "grad_formula = torch.where(x > 0, 1.0, 0.0)\n",
    "grad_formula[x == 0] = 0.5\n",
    "print(grad_formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef985f68",
   "metadata": {},
   "source": [
    "Exercise 4. \n",
    "\n",
    "Write in python a function to plot the sigmoid activation function and its gradient using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c645aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def plot_sigmoid():\n",
    "    x = torch.linspace(-10, 10, 100)\n",
    "    y = sigmoid(x)\n",
    "    y_grad = sigmoid_grad(x)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n",
    "    ax.plot(x, y, 'pink', label='function')\n",
    "    ax.plot(x, y_grad, 'blue', label='function gradient')\n",
    "    ax.set_xlabel('x')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51202a51",
   "metadata": {},
   "source": [
    "Exercise 5. \n",
    "\n",
    "Write in python a function to plot the ReLU activation function and its gradient using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e49c47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m     plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[0;32m     17\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 20\u001b[0m plot_leaky_relu()\n",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m, in \u001b[0;36mplot_leaky_relu\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_leaky_relu\u001b[39m():\n\u001b[1;32m----> 8\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      9\u001b[0m     y \u001b[38;5;241m=\u001b[39m leaky_relu(x)\n\u001b[0;32m     10\u001b[0m     y_grad \u001b[38;5;241m=\u001b[39m leaky_relu_grad(x)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return torch.maximum(torch.tensor(0), x)\n",
    "\n",
    "def relu_grad(x):\n",
    "    return torch.where(x > 0, 1, 0)\n",
    "\n",
    "def plot_relu():\n",
    "    x = torch.linspace(-10, 10, 100)\n",
    "    y = relu(x)\n",
    "    y_grad = relu_grad(x)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n",
    "    ax.plot(x, y, 'pink', label='function')\n",
    "    ax.plot(x, y_grad, 'blue', label='function gradient')\n",
    "    ax.set_xlabel('x')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81684ff",
   "metadata": {},
   "source": [
    "Exercise 6. \n",
    "\n",
    "Write in python a function to plot the tanh activation function and its gradient using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def tanh_grad(x):\n",
    "    return 1 - torch.square(torch.tanh(x))\n",
    "\n",
    "def plot_tanh():\n",
    "    x = torch.linspace(-10, 10, 100)\n",
    "    y = tanh(x)\n",
    "    y_grad = tanh_grad(x)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n",
    "    ax.plot(x, y, 'pink',  label='function')\n",
    "    ax.plot(x, y_grad, 'blue', label='function gradient')\n",
    "    ax.set_xlabel('x')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8740a0a8",
   "metadata": {},
   "source": [
    "Exercise 7. \n",
    "\n",
    "Write in python a function to plot the leaky ReLU activation function and its gradient using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b455646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.1):\n",
    "    return torch.maximum(alpha * x, x)\n",
    "\n",
    "def leaky_relu_grad(x, alpha=0.1):\n",
    "    return torch.where(x > 0, 1, alpha)\n",
    "\n",
    "def plot_leaky_relu():\n",
    "    x = torch.linspace(-10, 10, 100)\n",
    "    y = leaky_relu(x)\n",
    "    y_grad = leaky_relu_grad(x)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n",
    "    ax.plot(x, y, 'pink',  label='function')\n",
    "    ax.plot(x, y_grad, 'blue',  label='function gradient')\n",
    "    ax.set_xlabel('x')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_leaky_relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33119c",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "We define a class called *Perceptron* that inherits from *torch.nn.Module*. \n",
    "\n",
    "In the constructor, we define a single fully-connected linear layer with $input_dim$ inputs and $output_dim$ outputs, and a $sigmoid$ activation function. In the forward method, we apply the linear transformation to the input $x$, and then apply the sigmoid activation function to the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa86d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "\n",
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a178820e",
   "metadata": {},
   "source": [
    " We create an instance of this model and use it to make predictions like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78513e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(input_size, output_size)\n",
    "x = torch.tensor([0.5, 0.2])\n",
    "y = perceptron(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54070b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "optimizer = torch.optim.SGD(perceptron.parameters(), lr=0.1)  # Stochastic gradient descent optimizer\n",
    "\n",
    "# Generate some random input data and labels\n",
    "input_data = torch.randn((10, input_size))\n",
    "labels = torch.randint(0, 2, (10, output_size)).float()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = perceptron(input_data)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b840f4",
   "metadata": {},
   "source": [
    "Exercise 8: \n",
    "\n",
    "Implement a binary classification model using the Perceptron class in PyTorch for the logic OR. \n",
    "\n",
    "Your task is to create a Perceptron instance and train it using a proper  dataset and the binary cross-entropy loss with stochastic gradient descent optimizer. \n",
    "\n",
    "Here are the steps you can follow:\n",
    "\n",
    "Define a Perceptron class that inherits from torch.nn.Module and implements a binary classification model.\n",
    "\n",
    "Define a binary cross-entropy loss function using the torch.nn.BCEWithLogitsLoss module.\n",
    "\n",
    "Define a stochastic gradient descent optimizer using the torch.optim.SGD module.\n",
    "\n",
    "Train the Perceptron model on the training set using the binary cross-entropy loss and stochastic gradient descent optimizer.\n",
    "\n",
    "Evaluate the trained model compute the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3c5d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.2839\n",
      "Epoch [200/1000], Loss: 0.2278\n",
      "Epoch [300/1000], Loss: 0.1900\n",
      "Epoch [400/1000], Loss: 0.1624\n",
      "Epoch [500/1000], Loss: 0.1414\n",
      "Epoch [600/1000], Loss: 0.1250\n",
      "Epoch [700/1000], Loss: 0.1118\n",
      "Epoch [800/1000], Loss: 0.1010\n",
      "Epoch [900/1000], Loss: 0.0920\n",
      "Epoch [1000/1000], Loss: 0.0844\n",
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.1763],\n",
      "        [0.9310],\n",
      "        [0.9317],\n",
      "        [0.9988]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<RoundBackward0>)\n",
      "Test Accuracy: 1.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        #self.activation = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        #x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "input_size = 2\n",
    "output_size = 1\n",
    "\n",
    "perceptron = Perceptron(input_size, output_size)\n",
    "criterion = nn.BCEWithLogitsLoss() # loss function \n",
    "optimizer = torch.optim.SGD(perceptron.parameters(), lr=0.1) # optimizer\n",
    "\n",
    "input_data = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "labels = torch.tensor([[0.], [1.], [1.], [1.]])\n",
    "\n",
    "\n",
    "# train the Perceptron model on the training set using the binary cross-entropy loss and stochastic gradient descent optimizer\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass\n",
    "    outputs = perceptron(input_data)\n",
    "    loss = criterion(outputs, labels)\n",
    "    # backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print the loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "\n",
    "out = torch.nn.Sigmoid()(perceptron(input_data))\n",
    "predicted = torch.round(out)\n",
    "correct = (predicted == labels).sum()\n",
    "total = labels.size(0)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(input_data)\n",
    "print(out)\n",
    "print(predicted)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c043d-e892-4d2f-8576-f5e72102fcd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
